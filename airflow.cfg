[core]
# Exécuteur à utiliser (LocalExecutor pour un setup simple)
executor = LocalExecutor

# Connexion à la base de données (assurez-vous que PostgreSQL est configuré correctement)
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# Base de données (Airflow l'utilise pour stocker les métadonnées des DAGs)
sql_alchemy_pool_size = 5
sql_alchemy_max_overflow = 10
sql_alchemy_pool_recycle = 1800

# Le répertoire où les logs d'Airflow sont stockés
base_log_folder = /opt/airflow/logs

# Le répertoire des DAGs
dags_folder = /opt/airflow/dags

default_timezone = Europe/Paris

# Répertoire de travail pour Airflow
load_examples = False  # Empêcher de charger les exemples de DAGs

[webserver]
# Port du webserver
web_server_port = 8080
# Affichage des logs d'Airflow dans l'UI
web_server_workers = 4

[scheduler]
# Activation du scheduler
scheduler_queued_task_timeout = 600
scheduler_task_queued_timeout = 600
scheduler_task_queued_retry_delay = 5

[logging]
# Niveau de log
logging_level = INFO

[celery]
# Configuration Celery pour l'exécuteur, on n'en a pas besoin ici
# (utilisé dans des setups distribués, ne sert pas avec LocalExecutor)
